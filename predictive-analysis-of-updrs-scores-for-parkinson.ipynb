{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8898147,"sourceType":"datasetVersion","datasetId":5349767}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nTitle: Predictive Analysis of UPDRS Scores for Parkinson's Disease Severity\nDescription: This script processes the combined clinical, peptide, and protein data to train\n             a machine learning model for predicting the severity of Parkinson's disease\n             as measured by UPDRS scores. The script includes data preprocessing,\n             exploratory data analysis, model training and evaluation, hyperparameter tuning,\n             and SHAP values interpretation for model explainability.\n\"\"\"\n\n# Data manipulation and analysis\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.feature_selection import SelectFromModel\nfrom scipy.stats import randint, uniform\n\n# SHAP values for model interpretation\nimport shap\n\n# Loading the dataset\ndataset_file_path = '/kaggle/input/dataset/combined.csv'\ndata = pd.read_csv(dataset_file_path)\n\n# Table 1: Dataset Demographics and Clinical Characteristics\n# Used 'patient_id' and 'visit_month' has demographics for simplification\ndemographics = data[['patient_id', 'visit_month']].copy()\ndemographics['patient_id'] = demographics['patient_id'].astype(str)  # This step ensures patient_id is treated as a categorical variable\n\n# As per the understanding the 'visit_month' could correspond to 'disease_duration' in months\ndemographics.rename(columns={'visit_month': 'disease_duration_months'}, inplace=True)\n\n# Summary statistics for demographic data\ndemographics_summary = demographics.describe(include='all')\nprint(\"Table 1: Dataset Demographics and Clinical Characteristics\")\nprint(demographics_summary)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T08:49:17.625703Z","iopub.execute_input":"2024-07-08T08:49:17.626602Z","iopub.status.idle":"2024-07-08T08:49:17.722802Z","shell.execute_reply.started":"2024-07-08T08:49:17.626554Z","shell.execute_reply":"2024-07-08T08:49:17.720939Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Table 1: Dataset Demographics and Clinical Characteristics\n       patient_id  disease_duration_months\ncount        2615              2615.000000\nunique        248                      NaN\ntop         55096                      NaN\nfreq           17                      NaN\nmean          NaN                31.190822\nstd           NaN                25.199053\nmin           NaN                 0.000000\n25%           NaN                10.500000\n50%           NaN                24.000000\n75%           NaN                48.000000\nmax           NaN               108.000000\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Table 3: Statistical Summary of Key Variables\n# Using UPDRS scores and PeptideAbundance_mean has key variables\nkey_variables = data[['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4', 'PeptideAbundance_mean']].copy()\nkey_variables_summary = key_variables.describe()\nprint(\"\\nTable 3: Statistical Summary of Key Variables\")\nprint(key_variables_summary)","metadata":{"execution":{"iopub.status.busy":"2024-07-08T08:49:17.725541Z","iopub.execute_input":"2024-07-08T08:49:17.726093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# This is used to set the aesthetic style of the plots\nsns.set_style(\"whitegrid\")\n\n# Used Function to create distribution plots for UPDRS scores\ndef plot_updrs_distributions(data, updrs_columns):\n    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))\n    fig.suptitle('Distribution of UPDRS Scores', fontsize=16)\n\n    for ax, column in zip(axes.flatten(), updrs_columns):\n        sns.histplot(data[column], kde=True, ax=ax)\n        ax.set_title(f'Distribution of {column}')\n        ax.set_xlabel(column)\n        ax.set_ylabel('Count')\n\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # We can adjust the subplots to fit the figure area.\n\n# Used Function to create a correlation matrix heatmap\ndef plot_correlation_matrix(data, features):\n    corr = data[features].corr()\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', cbar_kws={'label': 'Correlation Coefficient'})\n    plt.title('Correlation Matrix of Numerical Features')\n\n# Used Function to create distribution plots for peptide and protein abundances\ndef plot_abundance_distributions(data, abundance_features):\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 6))\n    fig.suptitle('Distribution of Peptide and Protein Abundances', fontsize=16)\n\n    for ax, feature in zip(axes.flatten(), abundance_features):\n        sns.histplot(data[feature], bins=50, kde=True, ax=ax)\n        ax.set_title(f'Distribution of {feature}')\n        ax.set_xlabel(feature)\n        ax.set_ylabel('Count')\n\n    plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # here we can adjust the subplots to fit the figure area.\n\n# Updated plot_pairplot function to include a check for an empty DataFrame\ndef plot_pairplot(data, features, hue=None):\n    # This step helps us to drop missing values for pairplot and check if the resulting DataFrame is empty\n    pairplot_data = data[features].dropna()\n    if pairplot_data.empty:\n        print(f\"No data available to plot after dropping NaN values for features: {features}\")\n    else:\n        sns.pairplot(pairplot_data, hue=hue, diag_kind='kde')\n        plt.suptitle('Pair Plot of Selected Features and UPDRS Scores', fontsize=16)\n\n# Here we define the UPDRS columns for distribution plots\nupdrs_columns = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4']\n\n# Here we define the numerical features of the correlation matrix\nnumerical_features = ['updrs_1', 'updrs_2', 'updrs_3', 'updrs_4', 'PeptideAbundance_mean', 'NPX_mean']\n\n# Here we define the features of the abundance distributions\nabundance_features = ['PeptideAbundance_mean', 'NPX_mean']\n\n# Here we define the features of the pair plot\npairplot_features = ['visit_month', 'PeptideAbundance_mean', 'NPX_mean', 'updrs_1', 'updrs_2']\n\n# Now, calling the plotting functions with these defined variables\nplot_updrs_distributions(data, updrs_columns)\nplot_correlation_matrix(data, numerical_features)\nplot_abundance_distributions(data, abundance_features)\nplot_pairplot(data, pairplot_features + ['updrs_3'], hue='updrs_3')  # Adding 'updrs_3' to the pair plot features\n\nplt.show()  # This code helps to display all the plots generated so far\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing\n# Here we define the target variable and columns\ntarget = 'updrs_3'\ncategorical_cols = data.select_dtypes(include=['object', 'category']).columns.tolist()\nnumerical_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()\nnumerical_cols.remove(target)\n\n# This Ensures no missing values in the target variable before the train-test split\ndata = data.dropna(subset=[target])\n\n# Here we define features and target\nX = data.drop(columns=[target])\ny = data[target]\n    \n# Here we've split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Preprocessing pipeline\n# Note: We are using SimpleImputer only on features (X), not on the target (y)\ncolumn_transformer = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n        ('num', SimpleImputer(strategy='mean'), numerical_cols)\n    ],\n    remainder='passthrough'\n)\n\n# Machine learning pipelines\n# This step creates machine learning pipelines\nrf_pipeline = Pipeline(steps=[\n    ('preprocessor', column_transformer),\n    ('scaler', StandardScaler(with_mean=False)), # If features are sparse we can set with_mean=False\n    ('model', RandomForestRegressor(n_estimators=10, random_state=42, n_jobs=-1))\n])\n\nxgb_pipeline = Pipeline(steps=[\n    ('preprocessor', column_transformer),\n    ('scaler', StandardScaler(with_mean=False)), # If features are sparse we can set with_mean=False\n    ('model', XGBRegressor(n_estimators=10, random_state=42, n_jobs=-1))\n])\n\n# Model training and evaluation\n# This Performs cross-validation for RandomForest and XGBoost\nrf_cv_scores = cross_val_score(rf_pipeline, X_train, y_train, cv=5, scoring='r2')\nxgb_cv_scores = cross_val_score(xgb_pipeline, X_train, y_train, cv=5, scoring='r2')\n\n# Print cross-validation results\nprint(\"Random Forest CV R2 scores:\", rf_cv_scores)\nprint(\"Average Random Forest CV R2:\", rf_cv_scores.mean())\nprint(\"XGBoost CV R2 scores:\", xgb_cv_scores)\nprint(\"Average XGBoost CV R2:\", xgb_cv_scores.mean())\n\n# Selecting the best model based on cross-validation scores\nbest_pipeline = rf_pipeline if rf_cv_scores.mean() > xgb_cv_scores.mean() else xgb_pipeline\n# This step fits the best model on the training data\nbest_pipeline.fit(X_train, y_train)\n\n# Here we evaluate the best model on the test data\nfinal_predictions = best_pipeline.predict(X_test)\nfinal_rmse = np.sqrt(mean_squared_error(y_test, final_predictions))\nfinal_r2 = r2_score(y_test, final_predictions)\nprint(\"Final Model RMSE:\", final_rmse)\nprint(\"Final Model R2:\", final_r2)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scipy.sparse import issparse\n\n# This step helps in ensuring X_train_transformed and X_test_transformed are dense arrays if they are sparse\nX_train_transformed = best_pipeline.named_steps['preprocessor'].transform(X_train)\nif issparse(X_train_transformed):\n    X_train_transformed = X_train_transformed.toarray()\n\nX_test_transformed = best_pipeline.named_steps['preprocessor'].transform(X_test)\nif issparse(X_test_transformed):\n    X_test_transformed = X_test_transformed.toarray()\n\n# Creates the SHAP explainer using the transformed training set\nexplainer = shap.Explainer(best_pipeline.named_steps['model'], X_train_transformed)\n\n# This step calculates SHAP values using the transformed test set\nshap_values = explainer.shap_values(X_test_transformed)\n\n# Get feature names from the preprocessor step (after OneHotEncoding and other transformations)\nfeature_names_transformed = best_pipeline.named_steps['preprocessor'].get_feature_names_out()\n\n# This step plots the SHAP summary plot using the transformed feature names\nshap.summary_plot(shap_values, features=X_test_transformed, feature_names=feature_names_transformed)\n\n# Save the SHAP summary plot\nplt.savefig('shap_summary_plot.png')\n\n# This step plots SHAP dependence for 'updrs_2'\nshap.dependence_plot('num__updrs_2', shap_values, X_test_transformed, feature_names=feature_names_transformed)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We have used SelectFromModel as feature selection here\nselector = SelectFromModel(estimator=XGBRegressor(n_estimators=100, random_state=42), threshold='median')\nX_train_transformed = best_pipeline.named_steps['preprocessor'].transform(X_train)\nselector.fit(X_train_transformed, y_train)\n\n# Here we update the pipeline to include feature selection\nfinal_pipeline = Pipeline(steps=[\n    ('preprocessor', best_pipeline.named_steps['preprocessor']),\n    ('feature_selection', selector),\n    ('model', best_pipeline.named_steps['model'])\n])\n\n# Hyperparameter tuning with RandomizedSearchCV and addressed as param\nparam_distributions = {\n    'model__n_estimators': randint(50, 150),\n    'model__max_depth': randint(3, 10),\n    'model__learning_rate': uniform(0.01, 0.2)\n}\n\n# Initializing RandomizedSearchCV\nrandom_search = RandomizedSearchCV(\n    final_pipeline, \n    param_distributions=param_distributions, \n    n_iter=50, \n    scoring='r2', \n    cv=5, \n    random_state=42, \n    n_jobs=-1\n)\n\n# Fitting RandomizedSearchCV\nrandom_search.fit(X_train, y_train)\n\n# Print the best parameters and best score\nprint(\"Best Parameters:\", random_search.best_params_)\nprint(\"Best Score:\", random_search.best_score_)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting the best parameters for the model in the pipeline\nfinal_pipeline.set_params(model__learning_rate=0.14318447132349935, \n                          model__max_depth=6, \n                          model__n_estimators=135)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# In this step we are trying to fit the final model on the entire training data\nfinal_pipeline.set_params(**random_search.best_params_)\nfinal_pipeline.fit(X_train, y_train)\n\n# We make predictions on the test data with final model\nfinal_predictions = final_pipeline.predict(X_test)\nfinal_rmse = np.sqrt(mean_squared_error(y_test, final_predictions))\nfinal_r2 = r2_score(y_test, final_predictions)\nprint(\"Final Model RMSE:\", final_rmse)\nprint(\"Final Model R2:\", final_r2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}